# [Objects as Points](https://arxiv.org/abs/1904.07850)

## 0.背景

从 Faster RCNN 起就兴起了目标检测中的 anchor 热。如果要有一个理想的目标检测方法的话，那就是无须 anchor， 无须 nms， one stage 。这篇文章做到了，而且做得很好。

理由很简单，anchor 是先验知识，而且还需要在一个位置反复计算，本身就是对计算资源的浪费，nms同理，同一个物体识别多次，还要再花计算资源来抑制掉近似的框，最后才能得到想要的东西。

## 1. 思路

这篇文章是如何来解决这个问题的呢，答案是生成热力图。热力图的中心就是目标的中心。

### 1.1 输入输出

输入为 $R^{W*H*3}$ 的图片， 输出为  $[0,1]^{\frac{W}{R}*{\frac{H}{R}}*C}$ 。其中 W，H 是图像宽高，R 为输出热力图相对于原始图像降采样比例，C 通道数，在不同的任务中代表不同的意义，在目标检测中，代表类别个数。 输出表示 $Y_{x, y, c} = 1$ 表示检测到目标， $Y_{x, y, c} = 0$  表示为背景。

文中提到了几种 Encode-Decoder 框架来得到预测值 Y (也可以理解为热力图)。有 ResNet 18等，取得最优效果的网络为 DLA。主要操作就是对图像进行降采样后又进行升采样生成最后的热力图。

### 1.2 GT

GT 为高斯分布的热力图，并根据尺度自适应。

对于中心点(key point) $ p  \in R^2 $ 将 GT 转化为高斯分布的热力图

$Y_{xyz} = exp(-\frac{(x - p_{x})^2 + (x - p_{y})^2}{2\sigma_{p}^{2}})$ ，其中 $ \sigma_{p}$ 是目标尺度自适应的标准方差。 对于同类别且重叠的两个高斯函数，值取最大值。

### 1.3 Loss

热力图上的 Loss 是一个 focal loss
$$
L_{k}=\frac{1}{N}\sum_{xyz}\begin{cases}
(1-Y_{xyz})^{\alpha}log(Y_{xyz}),\quad Y_{xyz}=0 \\\\
(1-Y_{xyz})^{\beta}(Y_{xyz})^{\alpha}log(1-Y_{xyz}),\quad otherwise
\end{cases}
$$


其中 $ \alpha $ 和 $\beta$ 是 focal loss 的超参，作者设置为 2 和 4 。

N 是图像中关键点个数，除以 N 是为了将所有 focal loss 归一化。



由于图像下采样时会带来关键点位置的偏差，因此对中心点附加预测局部偏移 $ O \in R^{\frac{W}{R} * \frac{H}{R} * 2}$。也就是说增加两个通道来预测目标的 x, y 偏移值。

增加一个 L1 Loss 来训练偏移值，只对中心点位置做训练

$ L_{off} = \frac{1}{N}|O_{p} - (\frac{p}{R} - p) |$

## 2. 目标检测

前面的预测主要包括两个部分，一是 $L_{k}$ 预测了类别热力图，即类别分类。二是 $L_{off}$ 预测了中心点对于原始尺度的偏移值。

现在还需要预测目标的大小，即尺度。

假设目标 k 的 bbox 为 $(x_{1}^{k}, y_{1}^{k}, x_{2}^{k} ,y_{2}^{k} )$

则目标的中心位置，即 key point 为 $p_{k} = (\frac{x_{1}^{k} + x_{2}^{k}}{2}, \frac{y_{1}^{k} + y_{2}^{k}}{2})$

目标尺度为 $s_{k} = (x_{2}^{k} - x_{1}^{k}, y_{2}^{k} - y_{1}^{k})$

增加一个 L1 Loss 学习 位置预测

$ L_{size} = \frac{1}{N} \sum_{k=1}^{N} |S_{pk} - s{k}|$



最后目标检测的 Loss 函数为：

$ L_{det} = L_{k} + \lambda_{size}L_{size} + \lambda_{off}L_{off}$

作者采用的超参 $\lambda_{size}$ 和 $\lambda_{off}$ 采用的 0.1 和 1

网络最后层通道数为 C+4 ，其中类别 C 个，中心点偏移量 x, y 两个，制度 w, h 两个

## 3. 推理

  取每个热力图中大于周围八个值的峰值点，保留其中最大的100个值。然后预测的 x,y,w,h 直接计算出 bbox，而无需进行 NMS 等一系列操作。

## 4. 疑惑

推理的时候为什么取了100个极值直接算，也没提到再用阈值去滤掉低分极值，如果有两个极值挨得很近也不需要滤掉吗？不可能测试集上跑的这么完美吧，连两个连续极值都不会出现？作者对这部分的后处理只提到一句不需要 NMS 和计算 IOU。但是没有很详细介绍如何处理的，可能需要要再研究代码和结合实际问题来看吧。

## 5. 总结

这篇文章把目标检测的问题，再一次用一个相对比较简单的方法解决了。将目标检测问题近似为回归问题。这样在训练和后处理都能节省大量的代码时间。